# ollama模型部署
1. 用处：api_base_url = 'http://192.168.124.223:11434'  # ollama部署的文档解析embedding模型；
2. 部署流程：
	1. 登录221服务器：sqs @192.168.124.221 password
	2. Ollam的镜像就在路径：/mnt/hs-ssd/vllms/jisuan_backend
	3. 创建容器并启动：ocker run -it --name ollama --gpus all --net host -v /mnt/hs-ssd/vllms/:/mnt/hs-ssd/vllms -v /mnt/hs-ssd/vllms/ollama:/root/.ollama ollama/ollama:0.9.6
	4. 测试
		1. curl http://localhost:11434/api/tags 生成模型列表
		2. curl http://localhost:11434/api/version 查看版本


# chatchat 部署
## 1. 基础环境信息

- 部署服务器：`192.168.124.221`
- 账号：`sqs`
- 代码路径（宿主机）：
    `/mnt/hs-ssd/vllms/Langchain-Chatchat`
    该路径通过 Docker 挂载方式映射到容器内，使配置、代码可以直接修改并生效。【代码在宿主机，通过挂载到容器内】

## 2. Docker 容器管理

### 2.1 启动容器

第一次运行容器使用以下命令：
docker run -it --name chatchat-new-v1-auto --restart=always --gpus all --net host -v /mnt/hs-ssd/vllms:/mnt/hs-ssd/vllms --shm-size=256g chatchat:latest /mnt/hs-ssd/vllms/Langchain-Chatchat/start_chat.sh

说明：
- `--net host`：服务端口直接暴露
- `-v`：挂载宿主机目录（可随时修改配置文件） 容器：宿主机
- `--shm-size`：为大模型推理提供足够共享内存
- 最后的脚本为系统启动入口：`start_chat.sh`

停止：
	docker stop chatchat-new-v1-auto 
重新启动：
	docker restart chatchat-new-v1-auto 
修改配置文件后只需执行 restart 即可生效

## 3. 服务访问端口

启动后提供两个访问入口：

| 功能      | URL                                                          |
| ------- | ------------------------------------------------------------ |
| 对话功能    | [http://192.168.124.221:8501/](http://192.168.124.221:8501/) |
| 完整知识库功能 | [http://192.168.124.221:8502/](http://192.168.124.221:8502/) |

## 4. 知识库嵌入模型配置（Embedding）

此模块用于：
- 知识库文本向量化
- 相似度搜索
- 文档检索
### 修改文件路径

`/mnt/hs-ssd/vllms/Langchain-Chatchat/server/knowledge_base/kb_cache/base.py`

### 修改内容示例

```python
from langchain.embeddings.ollama import OllamaEmbeddings
embeddings = OllamaEmbeddings(base_url='http://192.168.124.221:11434', model="quentinz/bge-large-zh-v1.5:latest")
```
## 查看当前可用模型名称

在221服务器上运行：
`curl http://localhost:11434/api/tags`
可查看已加载的 Ollama 模型标签，按需替换 `model=` 字段。
## 5. 大语言模型（LLM）配置

此部分控制：
- 文本生成
- 问答
- 对话模型加载
### 修改文件路径

`/mnt/hs-ssd/vllms/Langchain-Chatchat/configs/model_config.py`

### 需要修改的配置示例

```python
    "openai-api-72b": {
        "model_name": "qwen3:32b-32k",
        "api_base_url": "http://192.168.124.221:11434/v1",
        "api_key": "1=1",
        "openai_api_key": "1=1",
        "openai_proxy": "",
     },
```

说明：
- `api_base_url` 指向 Ollama 接口
- `model_name` 必须与 Ollama 中加载的模型一致【ollama中加载的模型都可以】


# DiMS-DAPP部署
app.sqs.com服务器中/home/user/repo/DiMS-DAPP-Tracker路径下：
- 修改config.py文件
	- chat_url = "http://192.168.124.221:8501" 
	- 就可以在侧边栏展示问答系统chatchat
- 重启DiMs-DAPP服务，sudo systemctl restart dims-dapp-tracker.service
		


# kbase_dms后端
app.sqs.com服务器中/home/user/DiMS-DAPP/docker_home/kbase_dms路径下
- config.py文件修改接口
- 修改了代码
	- studio中点击停止再启动
镜像：
	- docker ps 中
	-  kbase_db_backend:3.12.3
	- 
