# ollama模型部署
1. 用处：api_base_url = 'http://192.168.124.223:11434'  # ollama部署的文档解析embedding模型；
2. 部署流程：
	1. 登录221服务器：sqs @192.168.124.221 password
	2. Ollam的镜像就在路径：/mnt/hs-ssd/vllms/jisuan_backend
	3. 创建容器并启动：docker run -it --name ollama --gpus all --net host -v /mnt/hs-ssd/vllms/:/mnt/hs-ssd/vllms -v /mnt/hs-ssd/vllms/ollama:/root/.ollama ollama/ollama:0.9.6
	4. 测试
		1. curl http://localhost:11434/api/tags 生成模型列表
[{"name":"qwen3:8b-32k","model":"qwen3:8b-32k","modified_at":"2025-08-20T18:20:18.493640689Z","size":5225388180,"digest":"14dffc2d257a494a8c04fc212c8c4db7ff83317cce66c685c68317c61f5619e9","details":{"parent_model":"","format":"gguf","family":"qwen3","families":["qwen3"],"parameter_size":"8.2B","quantization_level":"Q4_K_M"}},{"name":"qwen3:32b-32k","model":"qwen3:32b-32k","modified_at":"2025-08-20T18:14:32.430292824Z","size":20201253845,"digest":"ed5de41a6a247ed138b4bfbe2c3905ecb4f811b18ef869b6e047b0b6570911eb","details":{"parent_model":"","format":"gguf","family":"qwen3","families":["qwen3"],"parameter_size":"32.8B","quantization_level":"Q4_K_M"}},{"name":"qwen3:32b","model":"qwen3:32b","modified_at":"2025-08-20T16:10:20.020557741Z","size":20201253829,"digest":"030ee887880fc378860c2dd35101da424377520441ae4bfe7be6deff8ade7840","details":{"parent_model":"","format":"gguf","family":"qwen3","families":["qwen3"],"parameter_size":"32.8B","quantization_level":"Q4_K_M"}},{"name":"quentinz/bge-large-zh-v1.5:latest","model":"quentinz/bge-large-zh-v1.5:latest","modified_at":"2025-08-13T09:16:02.675658307Z","size":651022072,"digest":"bc8ca0995fcd2d324337e7e2958f115fc58a4da09e1684bc40a47aa95d5d79df","details":{"parent_model":"","format":"gguf","family":"bert","families":["bert"],"parameter_size":"324.47M","quantization_level":"F16"}},{"name":"qwen3:8b","model":"qwen3:8b","modified_at":"2025-07-16T04:55:22.143434198Z","size":5225388164,"digest":"500a1f067a9f782620b40bee6f7b0c89e17ae61f686b92c24933e4ca4b2b8b41","details":{"parent_model":"","format":"gguf","family":"qwen3","families":["qwen3"],"parameter_size":"8.2B","quantization_level":"Q4_K_M"}},{"name":"qwen2.5:7b","model":"qwen2.5:7b","modified_at":"2025-07-16T04:47:16.702560152Z","size":4683087332,"digest":"845dbda0ea48ed749caafd9e6037047aa19acfcfd82e704d7ca97d631a0b697e","details":{"parent_model":"","format":"gguf","family":"qwen2","families":["qwen2"],"parameter_size":"7.6B","quantization_level":"Q4_K_M"}}]}
		2. curl http://localhost:11434/api/version 查看版本


# chatchat 部署
## 1. 基础环境信息

- 部署服务器：`192.168.124.221`
- 账号：`sqs`
- 代码路径（宿主机）：
    `/mnt/hs-ssd/vllms/Langchain-Chatchat`
    该路径通过 Docker 挂载方式映射到容器内，使配置、代码可以直接修改并生效。【代码在宿主机，通过挂载到容器内】

## 2. Docker 容器管理

### 2.1 启动容器

第一次运行容器使用以下命令：
docker run -it --name chatchat-new-v1-auto --restart=always --gpus all --net host -v /mnt/hs-ssd/vllms:/mnt/hs-ssd/vllms --shm-size=256g chatchat:latest /mnt/hs-ssd/vllms/Langchain-Chatchat/start_chat.sh

说明：
- `--net host`：服务端口直接暴露
- `-v`：挂载宿主机目录（可随时修改配置文件） -v 宿主机路径:容器路径
- `--shm-size`：为大模型推理提供足够共享内存
- 最后的脚本为系统启动入口：`start_chat.sh`

停止：
	docker stop chatchat-new-v1-auto 
重新启动：
	docker restart chatchat-new-v1-auto 
修改配置文件后只需执行 restart 即可生效

## 3. 服务访问端口

启动后提供两个访问入口：

| 功能      | URL                                                          |
| ------- | ------------------------------------------------------------ |
| 对话功能    | [http://192.168.124.221:8501/](http://192.168.124.221:8501/) |
| 完整知识库功能 | [http://192.168.124.221:8502/](http://192.168.124.221:8502/) |

## 4. 知识库嵌入模型配置（Embedding）

此模块用于：
- 知识库文本向量化
- 相似度搜索
- 文档检索
### 修改文件路径

`/mnt/hs-ssd/vllms/Langchain-Chatchat/server/knowledge_base/kb_cache/base.py`

### 修改内容示例

```python
from langchain.embeddings.ollama import OllamaEmbeddings
embeddings = OllamaEmbeddings(base_url='http://192.168.124.221:11434', model="quentinz/bge-large-zh-v1.5:latest")
```
## 查看当前可用模型名称

在221服务器上运行：
`curl http://localhost:11434/api/tags`
可查看已加载的 Ollama 模型标签，按需替换 `model=` 字段。
## 5. 大语言模型（LLM）配置

此部分控制：
- 文本生成
- 问答
- 对话模型加载
### 修改文件路径

`/mnt/hs-ssd/vllms/Langchain-Chatchat/configs/model_config.py`

### 需要修改的配置示例

```python
    "openai-api-72b": {
        "model_name": "qwen3:32b-32k",
        "api_base_url": "http://192.168.124.221:11434/v1",
        "api_key": "1=1",
        "openai_api_key": "1=1",
        "openai_proxy": "",
     },
```

说明：
- `api_base_url` 指向 Ollama 接口
- `model_name` 必须与 Ollama 中加载的模型一致【ollama中加载的模型都可以】


# DiMS-DAPP部署
app.sqs.com服务器中/home/user/repo/DiMS-DAPP-Tracker路径下：
- 修改config.py文件
	- chat_url = "http://192.168.124.221:8501" 
	- 就可以在侧边栏展示问答系统chatchat
- 重启DiMs-DAPP服务，sudo systemctl restart dims-dapp-tracker.service
		


# kbase_dms后端
app.sqs.com服务器中/home/user/DiMS-DAPP/docker_home/kbase_dms路径下
- config.py文件修改接口
- 修改了代码
	- studio中点击停止再启动
镜像：
	- docker ps 中
	-  kbase_db_backend:3.12.3
	- 
